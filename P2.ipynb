{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/TattooeDeer/T3-ANN/blob/master/P2.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "jUgJ75rKw7GU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3C4LF4hgxDdg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pregunta 2\n"
      ]
    },
    {
      "metadata": {
        "id": "t5GkBlTRU-6c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "tYjI2RQPwnl4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "a3942819-cb47-43e1-993c-4f897cb6a70e"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(11235813)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sn\n",
        "\n",
        "# Tensorflow & Keras imports\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input, RepeatVector, TimeDistributed, Dense, Embedding, Flatten, Activation, Permute, Lambda, CuDNNGRU\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "# model saving\n",
        "!sudo apt-get install libhdf5-serial-dev\n",
        "import h5py\n",
        "\n",
        "from google.colab import files\n",
        "!git clone https://github.com/TattooeDeer/T3-ANN.git\n",
        "%cd T3-ANN\n",
        "!ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/f3/37504f07651330ddfdefa631ca5246974a60d0908216539efda842fd080f/gensim-3.5.0-cp36-cp36m-manylinux1_x86_64.whl (23.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.5MB 1.4MB/s \n",
            "\u001b[?25hCollecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/cf/3d/5f3a9a296d0ba8e00e263a8dee76762076b9eb5ddc254ccaa834651c8d65/smart_open-1.6.0.tar.gz\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 13.8MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/3d/54f063bc69911c2cc276fc4e39d5f44567eb48139debc3eee0d7428ee7b5/boto3-1.8.4-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 22.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.8.24)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 18.6MB/s \n",
            "\u001b[?25hCollecting botocore<1.12.0,>=1.11.4 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/4e/02ae5a3b77017707cb689f77c3227c93c70396aaa9e57469278731571f92/botocore-1.11.4-py2.py3-none-any.whl (4.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.6MB 6.4MB/s \n",
            "\u001b[?25hCollecting docutils>=0.10 (from botocore<1.12.0,>=1.11.4->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 20.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.12.0,>=1.11.4->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Building wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/73/f1/9b/ccf93d4ba073b6f79b1ed9df68ab5ce048d8136d0efcf90b30\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.8.4 botocore-1.11.4 bz2file-0.98 docutils-0.14 gensim-3.5.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/bin/sh: 1: sudo: not found\n",
            "Cloning into 'T3-ANN'...\n",
            "remote: Counting objects: 25, done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 25 (delta 9), reused 14 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n",
            "/content/T3-ANN\n",
            "attention_model_GRU.h5\t       Enunciado_T3.ipynb  README.md\t train_Q-A.csv\n",
            "attention_model_GRUWeights.h5  LICENSE\t\t   Tarea3.ipynb\n",
            "attention_model.h5\t       P2.ipynb\t\t   test_Q.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VL42xkHl4Jjf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## a) Carga de los datos en el entorno y análisis descriptivo"
      ]
    },
    {
      "metadata": {
        "id": "LY_FvybPw37X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "48641ba4-a0ac-4813-c7da-01fd51fa4f2a"
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train_Q-A.csv')\n",
        "test = pd.read_csv('test_Q.csv')\n",
        "\n",
        "print('Train shape: {0}'.format(train.shape))\n",
        "print('Test shape: {0}'.format(test.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (86821, 3)\n",
            "Test shape: (11873, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fu6HsKm5zWi5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b5821f9f-5f64-4a33-b424-8adc1fc61e5d"
      },
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>in the late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>singing and dancing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>Houston, Texas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>late 1990s</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         id  \\\n",
              "0  56be85543aeaaa14008c9063   \n",
              "1  56be85543aeaaa14008c9065   \n",
              "2  56be85543aeaaa14008c9066   \n",
              "3  56bf6b0f3aeaaa14008c9601   \n",
              "4  56bf6b0f3aeaaa14008c9602   \n",
              "\n",
              "                                            question               answer  \n",
              "0           When did Beyonce start becoming popular?    in the late 1990s  \n",
              "1  What areas did Beyonce compete in when she was...  singing and dancing  \n",
              "2  When did Beyonce leave Destiny's Child and bec...                 2003  \n",
              "3      In what city and state did Beyonce  grow up?        Houston, Texas  \n",
              "4         In which decade did Beyonce become famous?           late 1990s  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "bxh7VZd7w34l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "d2ca790b-ee90-4b7d-e640-27d88fdf3494"
      },
      "cell_type": "code",
      "source": [
        "train.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>86821</td>\n",
              "      <td>86821</td>\n",
              "      <td>86821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>86821</td>\n",
              "      <td>86769</td>\n",
              "      <td>64763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>573386274776f41900660c91</td>\n",
              "      <td>What year did Gladstone retire?</td>\n",
              "      <td>three</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>231</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              id                         question answer\n",
              "count                      86821                            86821  86821\n",
              "unique                     86821                            86769  64763\n",
              "top     573386274776f41900660c91  What year did Gladstone retire?  three\n",
              "freq                           1                                2    231"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "3rG5hv1Kzg8i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Al parecer el primer problema que observamos es que para preguntas cuya respuesta es cuantitativa, la respuesta dada es expresada en algunos casos en palabras y en otros en números, nuestro primer intento de solucionar esto será explorar si se cumplen patrones para cada tipo de respuesta, por ejemplo, solo las fechas se responden con números u otra regla similar.\n"
      ]
    },
    {
      "metadata": {
        "id": "pVnKEusYw31f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "6c65b505-e906-41cc-c2e1-47bdfb29207f"
      },
      "cell_type": "code",
      "source": [
        "test.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>11873</td>\n",
              "      <td>11873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>11873</td>\n",
              "      <td>11864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>5a5915043e1742001a15cf6c</td>\n",
              "      <td>What are the main sources of primary law?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              id                                   question\n",
              "count                      11873                                      11873\n",
              "unique                     11873                                      11864\n",
              "top     5a5915043e1742001a15cf6c  What are the main sources of primary law?\n",
              "freq                           1                                          2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "4tkbfa8izgFj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Se observa algo interesante analizando la cantidad de valores únicos y de conteo de las columnas `question` y  `answer`: La cantidad de ocurrencias únicas no es igual a la cantidad de registros totales, lo que puede indicar que quizás hay preguntas/respuestas repetidas en el dataset. Se observa un comportamiento similar en el conjunto de test aunque en mucho menor medida"
      ]
    },
    {
      "metadata": {
        "id": "MUg026sAzgIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "7c83ed2f-73c5-4e87-ccde-82553c3aa47e"
      },
      "cell_type": "code",
      "source": [
        "print('10 preguntas más populares:\\n')\n",
        "print(train[\"question\"].value_counts().head(10))\n",
        "print('\\n ----------------------------------------------------------\\n')\n",
        "print('10 respuestas más populares:\\n')\n",
        "print(train[\"answer\"].value_counts().head(10))\n",
        "print('\\n ----------------------------------------------------------\\n')\n",
        "print('10 preguntas más populares (Conjunto de test):\\n')\n",
        "print(test[\"question\"].value_counts().head(10))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 preguntas más populares:\n",
            "\n",
            "What year did Gladstone retire?                     2\n",
            "What is the state song of New York?                 2\n",
            "How many people were injured?                       2\n",
            "When did Nasser die?                                2\n",
            "Who first observed the photoelectric effect?        2\n",
            "What country was the largest aid donor to China?    2\n",
            "When did the Sex Pistols break up?                  2\n",
            "When did Japan invade Manchuria?                    2\n",
            "What type of religion is Buddhism?                  2\n",
            "Where was Chopin's last public performance?         2\n",
            "Name: question, dtype: int64\n",
            "\n",
            " ----------------------------------------------------------\n",
            "\n",
            "10 respuestas más populares:\n",
            "\n",
            "three    231\n",
            "two      206\n",
            "four     171\n",
            "five     133\n",
            "six       90\n",
            "2007      87\n",
            "2006      85\n",
            "2010      75\n",
            "seven     71\n",
            "2009      71\n",
            "Name: answer, dtype: int64\n",
            "\n",
            " ----------------------------------------------------------\n",
            "\n",
            "10 preguntas más populares (Conjunto de test):\n",
            "\n",
            "What are the main sources of primary law?                2\n",
            "In what sector are jobs beginning to increase?           2\n",
            "Who conceptualized the piston?                           2\n",
            "In what sector are jobs beginning to decrease?           2\n",
            "When was James Hutton born?                              2\n",
            "What is the population of Los Angeles?                   2\n",
            "Who designed Salamanca?                                  2\n",
            "Where does heat rejection occur in the Rankine cycle?    2\n",
            "What is the CJEU's duty?                                 2\n",
            "Who made Robert of Jumieges earl of Hereford?            1\n",
            "Name: question, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qx826njWzgKh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Las preguntas que más se repiten lo hacen a lo más 2 veces cada una. En cuanto a las respuestas, predominan aquellas que son números escritos como palabras, asi como también números escritos como tal los cuales aprecen ser fechas debido a la cantidad de dígitos y la magnitud que presentan.\n",
        "\n",
        "Lo anterior puede ser en respuesta a un sesgo en la selección de preguntas.\n",
        "\n",
        "Finalmente, se debe notar que el conjunto de test está compuesto solo por preguntas, lo que significa que, de utilizar los conjuntos de la forma en la que están, tendremos un entrenamiento supervisado pero tendremos que encontrar una métrica nueva para evaluar el desempeño final de la máquina.\n",
        "\n",
        "## b) Preprocesamiento\n",
        "\n",
        "Ahora se procederá a preprocesar ambos conjuntos con el objetivo de mejorar el desempeño que tenga la futura máquina al ingerirlos en el entrenamiento.\n",
        "Se _tokenizarán_ las preguntas y respuestas del conjunto de entrenamiento y de test, no realizando mayor modificación de las palabras dado que después necesitaremos reconstruir las oraciones."
      ]
    },
    {
      "metadata": {
        "id": "MnaFR0FIw3yF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_questions = [word_tokenize(sentence.lower()) for sentence in train[\"question\"]] #or processing\n",
        "test_questions = [word_tokenize(sentence.lower()) for sentence in  test[\"question\"]]\n",
        "train_answers = [word_tokenize(sentence) for sentence in train[\"answer\"]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bSPwafIP39XZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## c) Vocabulario\n",
        "\n",
        "Ahora, se procede a crear un vocabulario para codificar las palabras en las respuestas a generar, esta aproximación nos servirá para paliar el problema mencionado en el punto *a)*, de que no tenemos las respuestas correctas para el conjunto de test."
      ]
    },
    {
      "metadata": {
        "id": "5j3u0M6n39aE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "671be93b-27af-424b-dbe6-3f549bccfa09"
      },
      "cell_type": "code",
      "source": [
        "# Respuestas\n",
        "vocab_answer = set()\n",
        "for sentence in train_answers:\n",
        "  for word in sentence:\n",
        "    vocab_answer.add(word)\n",
        "vocab_answer = [\"#end\"] + list(vocab_answer)\n",
        "print('Posibles palabras para respuestas: ', len(vocab_answer))\n",
        "vocabA_indices = {c: i for i, c in enumerate(vocab_answer)}\n",
        "indices_vocabA = {i: c for i, c in enumerate(vocab_answer)}\n",
        "\n",
        "# Preguntas: Train\n",
        "vocab_question = set()\n",
        "for sentence in train_questions:\n",
        "  for word in sentence:\n",
        "    vocab_question.add(word)\n",
        "vocab_question = [\"#end\"] + list(vocab_question)\n",
        "print('Posibles palabras para preguntas (train): ', len(vocab_question))\n",
        "vocabQTrain_indices = {c: i for i, c in enumerate(vocab_question)}\n",
        "indices_vocabQTrain = {i: c for i, c in enumerate(vocab_question)}\n",
        "\n",
        "print('Diferencia en la cantidad de palabras que componen las preguntas y respuestas (train sets): ', \n",
        "      abs(len(vocab_answer) - len(vocab_question)))\n",
        "\n",
        "# Preguntas: Test\n",
        "vocab_question = set()\n",
        "for sentence in test_questions:\n",
        "  for word in sentence:\n",
        "    vocab_question.add(word)\n",
        "vocab_question = [\"#end\"] + list(vocab_question)\n",
        "print('Posibles palabras para preguntas (test): ', len(vocab_question))\n",
        "vocabQTest_indices = {c: i for i, c in enumerate(vocab_question)}\n",
        "indices_vocabQTest = {i: c for i, c in enumerate(vocab_question)}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Posibles palabras para respuestas:  47423\n",
            "Posibles palabras para preguntas (train):  39482\n",
            "Diferencia en la cantidad de palabras que componen las preguntas y respuestas (train sets):  7941\n",
            "Posibles palabras para preguntas (test):  10322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a32CLkBA39cc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "El vocabulario de palabras que componen las respuestas tiene _7941_ elementos más que el que compone las preguntas, esto puede hacer que hayan palabras encontradas en preguntas asociadas a varias palabras de respuesta, haciendo más dificil el discernir la respuesta correcta. Por otro lado, se debe notar la pequeña cantidad de palabras que componen el vocabulario de test."
      ]
    },
    {
      "metadata": {
        "id": "g5mWynT339fD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## d) Codificación de tokens y padding\n",
        "\n",
        "Aplicaremos una codificación tipo *one-hot vector* sobre los tokens, calcularemos el largo máximo que puede tener una respuesta y una pregunta y reformularemos las secuencias de entrada del modelo agregandoles un padding al final, esto hará que el tamaño de input sea constante. Para las preguntas se rellenará con '0' (recordar que las palabras estan indexadas y tokenizadas), mientras que para las respuestas se rellenará con el carácter definido *'#end'* que indica cuando la pregunta ha sido respondida.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "gWfUQCX139jE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# input and output to onehotvector\n",
        "X_answers = [[vocabA_indices[palabra] for palabra in sentence] for sentence in train_answers]\n",
        "X_test_Q = [[vocabQTest_indices[palabra] for palabra in sentence] for sentence in test_questions]\n",
        "X_train_Q = [[vocabQTrain_indices[palabra] for palabra in sentence] for sentence in train_questions]\n",
        "\n",
        "# padding\n",
        "max_input_length = np.max(list(map(len, train_questions)))\n",
        "max_output_length = np.max(list(map(len, train_answers)))\n",
        "\n",
        "X_train_Q = sequence.pad_sequences(X_train_Q, maxlen = max_input_length,\n",
        "                                        padding = 'post', value = 0)\n",
        "X_test_Q = sequence.pad_sequences(X_test_Q, maxlen = max_input_length,\n",
        "                                        padding = 'post', value = 0)\n",
        "X_answers = sequence.pad_sequences(X_answers, maxlen = max_output_length,\n",
        "                                        padding = 'post', value = vocabA_indices['#end'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y985lm4O39lm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## e) Modelo *Encoder-Decoder* con módulos de atención\n",
        "\n",
        "Utilizaremos un encoder basado en GRU."
      ]
    },
    {
      "metadata": {
        "id": "x6DiPDzP39oE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Encoder-Decoder modelo\n",
        "length_output = max_output_length\n",
        "hidden_dim = 128\n",
        "\n",
        "embedding_vector = 64\n",
        "encoder_input = Input(shape = (max_input_length, ))\n",
        "embedded = Embedding(input_dim = len(vocabQTrain_indices), output_dim = embedding_vector,\n",
        "                    input_length = max_input_length)(encoder_input)\n",
        "encoder = CuDNNGRU(hidden_dim, return_sequences = True)(embedded)\n",
        "\n",
        "attention = TimeDistributed(Dense(max_output_length, activation = 'tanh'))(encoder)\n",
        "\n",
        "# softmax a las atenciones sobre todo T\n",
        "attention = Permute([2, 1])(attention)\n",
        "attention = Activation('softmax')(attention)\n",
        "attention = Permute([2, 1])(attention)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_t_mw8tR39qx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Aplicacion de la atencion al modelo\n",
        "def attention_multiply(vects):\n",
        "  encoder, attention = vects\n",
        "  return K.batch_dot(attention, encoder, axes = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fhK96YZB39hg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sent_representation = Lambda(attention_multiply)([encoder, attention])\n",
        "decoder = CuDNNGRU(hidden_dim, return_sequences= True)(sent_representation)\n",
        "probabilities = TimeDistributed(Dense(len(vocab_answer), activation = 'softmax'))(decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tNQ7RXP1XcTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "9d0ef6c6-18c6-4802-8541-495acfef82c6"
      },
      "cell_type": "code",
      "source": [
        "model = Model(encoder_input, probabilities)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 60)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 60, 64)       2526848     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnngru_1 (CuDNNGRU)          (None, 60, 128)      74496       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 60, 46)       5934        cu_dnngru_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 46, 60)       0           time_distributed_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 46, 60)       0           permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "permute_2 (Permute)             (None, 60, 46)       0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 46, 128)      0           cu_dnngru_1[0][0]                \n",
            "                                                                 permute_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnngru_2 (CuDNNGRU)          (None, 46, 128)      99072       lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 46, 47423)    6117567     cu_dnngru_2[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 8,823,917\n",
            "Trainable params: 8,823,917\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dfmoHD1_YVU0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## f) Entrenamiento del modelo\n",
        "\n",
        "Se entrenará el modelo con 10 epochs con tamaño de batch 64"
      ]
    },
    {
      "metadata": {
        "id": "adYhTEB2YtX5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_answers = X_answers.reshape(X_answers.shape[0], X_answers.shape[1],1)\n",
        "X_answers.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bzDkoGONY8Cu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "90c47fa2-6920-4f40-fd56-07b610e36185"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "BS = 64\n",
        "model.fit(X_train_Q, X_answers, epochs = 10, batch_size = BS,\n",
        "               validation_split = 0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 69456 samples, validate on 17365 samples\n",
            "Epoch 1/10\n",
            "69456/69456 [==============================] - 356s 5ms/step - loss: 0.9792 - val_loss: 0.8078\n",
            "Epoch 2/10\n",
            "69456/69456 [==============================] - 353s 5ms/step - loss: 0.6997 - val_loss: 0.8136\n",
            "Epoch 3/10\n",
            "69456/69456 [==============================] - 352s 5ms/step - loss: 0.6826 - val_loss: 0.8227\n",
            "Epoch 4/10\n",
            "69456/69456 [==============================] - 352s 5ms/step - loss: 0.6690 - val_loss: 0.8304\n",
            "Epoch 5/10\n",
            "69456/69456 [==============================] - 352s 5ms/step - loss: 0.6557 - val_loss: 0.8415\n",
            "Epoch 6/10\n",
            "69456/69456 [==============================] - 352s 5ms/step - loss: 0.6383 - val_loss: 0.8402\n",
            "Epoch 7/10\n",
            "69456/69456 [==============================] - 352s 5ms/step - loss: 0.6184 - val_loss: 0.8483\n",
            "Epoch 8/10\n",
            "69456/69456 [==============================] - 352s 5ms/step - loss: 0.6006 - val_loss: 0.8469\n",
            "Epoch 9/10\n",
            "69456/69456 [==============================] - 352s 5ms/step - loss: 0.5834 - val_loss: 0.8569\n",
            "Epoch 10/10\n",
            "69456/69456 [==============================] - 352s 5ms/step - loss: 0.5667 - val_loss: 0.8669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6d07dc1fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "9-0aZ2d359MB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_krsO_gfFOdD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_9_vauVFFOm6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad677a3f-2c57-4f7f-a95d-7ec642352419"
      },
      "cell_type": "code",
      "source": [
        "max_output_length"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "4Q2s2436ZNDR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## g) Predicción del modelo\n",
        "\n",
        "Evaluaremos ahora las predicciones del modelo a traves del modelamiento de la distribución de probabilidad de las respuestas, basandonos en la frecuencia de ocurrencia de los tokens encontrados en las mismas.\n"
      ]
    },
    {
      "metadata": {
        "id": "TFpg3tZ__uWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9b17cd3-74ec-4584-b736-209eeafbf7b0"
      },
      "cell_type": "code",
      "source": [
        "model.predict(X_train_Q[1:2]).shape\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 46, 47423)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "BDdUwUMJgpnl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "c462130f-f60c-4917-c224-3551e9377735"
      },
      "cell_type": "code",
      "source": [
        "def predict_words(model, example, diversity):\n",
        "  model = load_model('att.h5')\n",
        "  example = np.array(example)\n",
        "  example.reshape((60,))\n",
        "  prediction = model.predict(example)\n",
        "  return prediction\n",
        "  \n",
        "n = 10\n",
        "for i in range(n):\n",
        "  indexs = np.random.randint(0, len(X_test_Q)-2)\n",
        "  example = X_train_Q[indexs:(indexs+1)]\n",
        "  indexes_answer = predict_words(model, example, 0.85)\n",
        "  question = test['question'][indexs]\n",
        "  print('Pregunta: ', question)\n",
        "  answer = ''\n",
        "  for index in indexes_answer:\n",
        "    print(index)\n",
        "    if (indices_vocabA[index] == '#end'): # fin de la oracion\n",
        "      continue\n",
        "    else:\n",
        "      answer += indices_vocabA[index]+' '\n",
        "  print('Respuesta: ', answer)\n",
        "print('Los ha predecido todos!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pregunta:  Which coast is the desert on?\n",
            "[[4.1513899e-04 5.2539172e-06 1.3842223e-04 ... 2.8577507e-08\n",
            "  6.0705224e-07 7.0327583e-06]\n",
            " [1.8703482e-01 2.5425513e-06 4.3556163e-05 ... 2.0128969e-08\n",
            "  4.7363910e-06 9.8626877e-08]\n",
            " [4.1331157e-01 2.3604442e-05 3.1975451e-05 ... 3.0457795e-08\n",
            "  7.3399292e-07 9.7371384e-08]\n",
            " ...\n",
            " [9.9996328e-01 6.3393166e-11 2.7207253e-10 ... 5.4303899e-14\n",
            "  1.6367563e-13 1.5946318e-13]\n",
            " [9.9996471e-01 6.2028306e-11 2.6252076e-10 ... 5.3592239e-14\n",
            "  1.6394205e-13 1.5341913e-13]\n",
            " [9.9996340e-01 6.1126576e-11 2.6784938e-10 ... 5.2262211e-14\n",
            "  1.5672468e-13 1.5598361e-13]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-16f3be0f396c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes_answer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices_vocabA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'#end'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# fin de la oracion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MagGuISOPdnU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('att.h5')\n",
        "model.save_weights('attW.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v8zMagSuqsp1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## h) Evaluacion del modelo\n",
        "Para verificar la calidad del modelo, compararemos con el benchmark"
      ]
    },
    {
      "metadata": {
        "id": "p_1NO7R9PdGB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ffqf1bH-qsoI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python evaluate-v2.0.py dev-v2.0.json predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X_awuqAZqslg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dic_predictions = {}\n",
        "for example, id_e in zip(Xtest_question, df_test[\"id\"]): # todos los ejemplos\n",
        "  indexes_answer = predict_words(model, example) # predice palabra en cada instante\n",
        "  answer = \"\"\n",
        "  for index in indexes_answer:\n",
        "    if(indices_vocabA[index] == '#end'): # Final de la oracion\n",
        "      continue\n",
        "    else:\n",
        "      answer += indices_vocabA[index]+\" \"\n",
        "  dic_predictions[id_e] = answer\n",
        "  contador += 1\n",
        "  print('Los ha predecido todos!')\n",
        "  json_save = json.dumps(dic_predictions)\n",
        "  archivo = open('predictions', 'w')\n",
        "  archivo.write(json.save)\n",
        "  archivo.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
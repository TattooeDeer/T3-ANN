{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUgJ75rKw7GU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3C4LF4hgxDdg"
   },
   "source": [
    "# Pregunta 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "colab_type": "code",
    "id": "tYjI2RQPwnl4",
    "outputId": "d1c05ae0-2556-49f1-f36b-6b10acf8f04b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ignacio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "#!pip install gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(11235813)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sn\n",
    "\n",
    "# Tensorflow & Keras imports\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, RepeatVector, TimeDistributed, Dense, Embedding, Flatten, Activation, Permute, Lambda, GRU\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# model saving\n",
    "#!sudo apt-get install libhdf5-serial-dev\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VL42xkHl4Jjf"
   },
   "source": [
    "## a) Carga de los datos en el entorno y análisis descriptivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LY_FvybPw37X",
    "outputId": "0c0fe0d4-ee3d-4d2f-eda4-b286d8f62e93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (86821, 3)\n",
      "Test shape: (11873, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train_Q-A.csv')\n",
    "test = pd.read_csv('test_Q.csv')\n",
    "\n",
    "print('Train shape: {0}'.format(train.shape))\n",
    "print('Test shape: {0}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "fu6HsKm5zWi5",
    "outputId": "0b4dff42-1482-46c6-c323-ce86eab655a6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Houston, Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>late 1990s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  56be85543aeaaa14008c9063   \n",
       "1  56be85543aeaaa14008c9065   \n",
       "2  56be85543aeaaa14008c9066   \n",
       "3  56bf6b0f3aeaaa14008c9601   \n",
       "4  56bf6b0f3aeaaa14008c9602   \n",
       "\n",
       "                                            question               answer  \n",
       "0           When did Beyonce start becoming popular?    in the late 1990s  \n",
       "1  What areas did Beyonce compete in when she was...  singing and dancing  \n",
       "2  When did Beyonce leave Destiny's Child and bec...                 2003  \n",
       "3      In what city and state did Beyonce  grow up?        Houston, Texas  \n",
       "4         In which decade did Beyonce become famous?           late 1990s  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "bxh7VZd7w34l",
    "outputId": "a177c5e4-f001-425e-f5e4-63ca60461e1f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>86821</td>\n",
       "      <td>86821</td>\n",
       "      <td>86821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>86821</td>\n",
       "      <td>86769</td>\n",
       "      <td>64763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>572673e2f1498d1400e8e015</td>\n",
       "      <td>How long was Chopin's funeral delayed?</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id                                question  \\\n",
       "count                      86821                                   86821   \n",
       "unique                     86821                                   86769   \n",
       "top     572673e2f1498d1400e8e015  How long was Chopin's funeral delayed?   \n",
       "freq                           1                                       2   \n",
       "\n",
       "       answer  \n",
       "count   86821  \n",
       "unique  64763  \n",
       "top     three  \n",
       "freq      231  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3rG5hv1Kzg8i"
   },
   "source": [
    "Al parecer el primer problema que observamos es que para preguntas cuya respuesta es cuantitativa, la respuesta dada es expresada en algunos casos en palabras y en otros en números, nuestro primer intento de solucionar esto será explorar si se cumplen patrones para cada tipo de respuesta, por ejemplo, solo las fechas se responden con números u otra regla similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "pVnKEusYw31f",
    "outputId": "6fc0724e-dccc-4fc7-f543-b156688c4d07"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11873</td>\n",
       "      <td>11873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>11873</td>\n",
       "      <td>11864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>5a67e5278476ee001a58a761</td>\n",
       "      <td>Who designed Salamanca?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id                 question\n",
       "count                      11873                    11873\n",
       "unique                     11873                    11864\n",
       "top     5a67e5278476ee001a58a761  Who designed Salamanca?\n",
       "freq                           1                        2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4tkbfa8izgFj"
   },
   "source": [
    "Se observa algo interesante analizando la cantidad de valores únicos y de conteo de las columnas `question` y  `answer`: La cantidad de ocurrencias únicas no es igual a la cantidad de registros totales, lo que puede indicar que quizás hay preguntas/respuestas repetidas en el dataset. Se observa un comportamiento similar en el conjunto de test aunque en mucho menor medida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "id": "MUg026sAzgIP",
    "outputId": "5b6ac03b-f2ae-4a9d-e389-b164cdc90e93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 preguntas más populares:\n",
      "\n",
      "How long was Chopin's funeral delayed?          2\n",
      "How many people were injured?                   2\n",
      "Who did Victoria marry?                         2\n",
      "What year did Chopin leave Warsaw?              2\n",
      "What was Whitehead's father's profession?       2\n",
      "Who wrote the Divine Comedy?                    2\n",
      "Who paid for Chopin's funeral?                  2\n",
      "How did Jan Hus die?                            2\n",
      "Which airport was shut down?                    2\n",
      "Who first observed the photoelectric effect?    2\n",
      "Name: question, dtype: int64\n",
      "\n",
      " ----------------------------------------------------------\n",
      "\n",
      "10 respuestas más populares:\n",
      "\n",
      "three    231\n",
      "two      206\n",
      "four     171\n",
      "five     133\n",
      "six       90\n",
      "2007      87\n",
      "2006      85\n",
      "2010      75\n",
      "2009      71\n",
      "seven     71\n",
      "Name: answer, dtype: int64\n",
      "\n",
      " ----------------------------------------------------------\n",
      "\n",
      "10 preguntas más populares (Conjunto de test):\n",
      "\n",
      "Who designed Salamanca?                                  2\n",
      "Who conceptualized the piston?                           2\n",
      "When was James Hutton born?                              2\n",
      "What is the population of Los Angeles?                   2\n",
      "In what sector are jobs beginning to decrease?           2\n",
      "Where does heat rejection occur in the Rankine cycle?    2\n",
      "What is the CJEU's duty?                                 2\n",
      "What are the main sources of primary law?                2\n",
      "In what sector are jobs beginning to increase?           2\n",
      "Why did the demand for rentals decrease?                 1\n",
      "Name: question, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('10 preguntas más populares:\\n')\n",
    "print(train[\"question\"].value_counts().head(10))\n",
    "print('\\n ----------------------------------------------------------\\n')\n",
    "print('10 respuestas más populares:\\n')\n",
    "print(train[\"answer\"].value_counts().head(10))\n",
    "print('\\n ----------------------------------------------------------\\n')\n",
    "print('10 preguntas más populares (Conjunto de test):\\n')\n",
    "print(test[\"question\"].value_counts().head(10))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qx826njWzgKh"
   },
   "source": [
    "Las preguntas que más se repiten lo hacen a lo más 2 veces cada una. En cuanto a las respuestas, predominan aquellas que son números escritos como palabras, asi como también números escritos como tal los cuales aprecen ser fechas debido a la cantidad de dígitos y la magnitud que presentan.\n",
    "\n",
    "Lo anterior puede ser en respuesta a un sesgo en la selección de preguntas.\n",
    "\n",
    "Finalmente, se debe notar que el conjunto de test está compuesto solo por preguntas, lo que significa que, de utilizar los conjuntos de la forma en la que están, tendremos un entrenamiento supervisado pero tendremos que encontrar una métrica nueva para evaluar el desempeño final de la máquina.\n",
    "\n",
    "## b) Preprocesamiento\n",
    "\n",
    "Ahora se procederá a preprocesar ambos conjuntos con el objetivo de mejorar el desempeño que tenga la futura máquina al ingerirlos en el entrenamiento.\n",
    "Se _tokenizarán_ las preguntas y respuestas del conjunto de entrenamiento y de test, no realizando mayor modificación de las palabras dado que después necesitaremos reconstruir las oraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MnaFR0FIw3yF"
   },
   "outputs": [],
   "source": [
    "train_questions = [word_tokenize(sentence.lower()) for sentence in train[\"question\"]] #or processing\n",
    "test_questions = [word_tokenize(sentence.lower()) for sentence in  test[\"question\"]]\n",
    "train_answers = [word_tokenize(sentence) for sentence in train[\"answer\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bSPwafIP39XZ"
   },
   "source": [
    "## c) Vocabulario\n",
    "\n",
    "Ahora, se procede a crear un vocabulario para codificar las palabras en las respuestas a generar, esta aproximación nos servirá para paliar el problema mencionado en el punto *a)*, de que no tenemos las respuestas correctas para el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "5j3u0M6n39aE",
    "outputId": "fdb91be4-ad58-491a-80ea-cdf3859ca335"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posibles palabras para respuestas:  47423\n",
      "Posibles palabras para preguntas (train):  39477\n",
      "Diferencia en la cantidad de palabras que componen las preguntas y respuestas (train sets):  7946\n",
      "Posibles palabras para preguntas (test):  10322\n"
     ]
    }
   ],
   "source": [
    "# Respuestas\n",
    "vocab_answer = set()\n",
    "for sentence in train_answers:\n",
    "    for word in sentence:\n",
    "        vocab_answer.add(word)\n",
    "vocab_answer = [\"#end\"] + list(vocab_answer)\n",
    "print('Posibles palabras para respuestas: ', len(vocab_answer))\n",
    "vocabA_indices = {c: i for i, c in enumerate(vocab_answer)}\n",
    "indices_vocabA = {i: c for i, c in enumerate(vocab_answer)}\n",
    "\n",
    "# Preguntas: Train\n",
    "vocab_question = set()\n",
    "for sentence in train_questions:\n",
    "    for word in sentence:\n",
    "        vocab_question.add(word)\n",
    "vocab_question = [\"#end\"] + list(vocab_question)\n",
    "print('Posibles palabras para preguntas (train): ', len(vocab_question))\n",
    "vocabQTrain_indices = {c: i for i, c in enumerate(vocab_question)}\n",
    "indices_vocabQTrain = {i: c for i, c in enumerate(vocab_question)}\n",
    "\n",
    "print('Diferencia en la cantidad de palabras que componen las preguntas y respuestas (train sets): ', \n",
    "      abs(len(vocab_answer) - len(vocab_question)))\n",
    "\n",
    "# Preguntas: Test\n",
    "vocab_question = set()\n",
    "for sentence in test_questions:\n",
    "    for word in sentence:\n",
    "        vocab_question.add(word)\n",
    "vocab_question = [\"#end\"] + list(vocab_question)\n",
    "print('Posibles palabras para preguntas (test): ', len(vocab_question))\n",
    "vocabQTest_indices = {c: i for i, c in enumerate(vocab_question)}\n",
    "indices_vocabQTest = {i: c for i, c in enumerate(vocab_question)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a32CLkBA39cc"
   },
   "source": [
    "El vocabulario de palabras que componen las respuestas tiene _7941_ elementos más que el que compone las preguntas, esto puede hacer que hayan palabras encontradas en preguntas asociadas a varias palabras de respuesta, haciendo más dificil el discernir la respuesta correcta. Por otro lado, se debe notar la pequeña cantidad de palabras que componen el vocabulario de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g5mWynT339fD"
   },
   "source": [
    "## d) Codificación de tokens y padding\n",
    "\n",
    "Aplicaremos una codificación tipo *one-hot vector* sobre los tokens, calcularemos el largo máximo que puede tener una respuesta y una pregunta y reformularemos las secuencias de entrada del modelo agregandoles un padding al final, esto hará que el tamaño de input sea constante. Para las preguntas se rellenará con '0' (recordar que las palabras estan indexadas y tokenizadas), mientras que para las respuestas se rellenará con el carácter definido *'#end'* que indica cuando la pregunta ha sido respondida.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWfUQCX139jE"
   },
   "outputs": [],
   "source": [
    "# input and output to onehotvector\n",
    "X_answers = [[vocabA_indices[palabra] for palabra in sentence] for sentence in train_answers]\n",
    "X_test_Q = [[vocabQTest_indices[palabra] for palabra in sentence] for sentence in test_questions]\n",
    "X_train_Q = [[vocabQTrain_indices[palabra] for palabra in sentence] for sentence in train_questions]\n",
    "\n",
    "# padding\n",
    "max_input_length = np.max(list(map(len, train_questions)))\n",
    "max_output_length = np.max(list(map(len, train_answers)))\n",
    "\n",
    "X_train_Q = sequence.pad_sequences(X_train_Q, maxlen = max_input_length,\n",
    "                                        padding = 'post', value = 0)\n",
    "X_test_Q = sequence.pad_sequences(X_test_Q, maxlen = max_input_length,\n",
    "                                        padding = 'post', value = 0)\n",
    "X_answers = sequence.pad_sequences(X_answers, maxlen = max_output_length,\n",
    "                                        padding = 'post', value = vocabA_indices['#end'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y985lm4O39lm"
   },
   "source": [
    "## e) Modelo *Encoder-Decoder* con módulos de atención\n",
    "\n",
    "Utilizaremos un encoder basado en GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6DiPDzP39oE"
   },
   "outputs": [],
   "source": [
    "# Encoder-Decoder modelo\n",
    "length_output = max_output_length\n",
    "hidden_dim = 128\n",
    "\n",
    "embedding_vector = 64\n",
    "encoder_input = Input(shape = (max_input_length, ))\n",
    "embedded = Embedding(input_dim = len(vocabQTrain_indices), output_dim = embedding_vector,\n",
    "                    input_length = max_input_length)(encoder_input)\n",
    "encoder = GRU(hidden_dim, return_sequences = True)(embedded)\n",
    "\n",
    "attention = TimeDistributed(Dense(max_output_length, activation = 'tanh'))(encoder)\n",
    "\n",
    "# softmax a las atenciones sobre todo T\n",
    "attention = Permute([2, 1])(attention)\n",
    "attention = Activation('softmax')(attention)\n",
    "attention = Permute([2, 1])(attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_t_mw8tR39qx"
   },
   "outputs": [],
   "source": [
    "# Aplicacion de la atencion al modelo\n",
    "def attention_multiply(vects):\n",
    "    encoder, attention = vects\n",
    "    return K.batch_dot(attention, encoder, axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fhK96YZB39hg"
   },
   "outputs": [],
   "source": [
    "sent_representation = Lambda(attention_multiply)([encoder, attention])\n",
    "decoder = GRU(hidden_dim, return_sequences= True)(sent_representation)\n",
    "probabilities = TimeDistributed(Dense(len(vocab_answer), activation = 'softmax'))(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "tNQ7RXP1XcTM",
    "outputId": "f1af9f90-8e9d-4be4-a5e6-88fffc3dc57c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 60, 64)       2526528     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     (None, 60, 128)      74112       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 60, 46)       5934        gru_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 46, 60)       0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 46, 60)       0           permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 60, 46)       0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 46, 128)      0           gru_3[0][0]                      \n",
      "                                                                 permute_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_4 (GRU)                     (None, 46, 128)      98688       lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 46, 47423)    6117567     gru_4[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 8,822,829\n",
      "Trainable params: 8,822,829\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(encoder_input, probabilities)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dfmoHD1_YVU0"
   },
   "source": [
    "## f) Entrenamiento del modelo\n",
    "\n",
    "Se entrenará el modelo con 10 epochs con tamaño de batch 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "adYhTEB2YtX5",
    "outputId": "748023fd-e020-4ad7-9157-5a0e5178cf89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86821, 46, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_answers = X_answers.reshape(X_answers.shape[0], X_answers.shape[1],1)\n",
    "X_answers.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzDkoGONY8Cu",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBS = 64\\nmodel.fit(X_train_Q, X_answers, epochs = 10, batch_size = BS,\\n               validation_split = 0.2)\\n               '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para evitar ejecutarla cuando reinicie el kernel\n",
    "\"\"\"\n",
    "BS = 64\n",
    "model.fit(X_train_Q, X_answers, epochs = 10, batch_size = BS,\n",
    "               validation_split = 0.2)\n",
    "               \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-0aZ2d359MB"
   },
   "outputs": [],
   "source": [
    "#model.save('attention_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_krsO_gfFOdD"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (truncated file: eof = 27364879, sblock->base_addr = 0, stored_eof = 35318668)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-89275d3b995c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model = load_model('attention_model_GRU.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attention_model_GRUWeights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   2656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2658\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2659\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (truncated file: eof = 27364879, sblock->base_addr = 0, stored_eof = 35318668)"
     ]
    }
   ],
   "source": [
    "#model = load_model('attention_model_GRU.h5')\n",
    "model.load_weights('attention_model_GRUWeights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "_9_vauVFFOm6",
    "outputId": "be1f0d18-e112-422b-c3e1-923e4c16a919"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cbc34c1d0cb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmax_output_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'max_output_length' is not defined"
     ]
    }
   ],
   "source": [
    "max_output_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Q2s2436ZNDR"
   },
   "source": [
    "## g) Predicción del modelo\n",
    "\n",
    "Evaluaremos ahora las predicciones del modelo a traves del modelamiento de la distribución de probabilidad de las respuestas, basandonos en la frecuencia de ocurrencia de los tokens encontrados en las mismas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TFpg3tZ__uWQ",
    "outputId": "a5bb091b-2c85-444f-ac96-4a247176f93d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 46, 47423)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train_Q[1:2]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "BDdUwUMJgpnl",
    "outputId": "37071b37-5a6f-4cd8-b824-f6e75782220b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta:  What kind of communication can be implemented?\n",
      "[[2.57918055e-05 4.86640431e-07 1.69777297e-06 ... 1.65017548e-06\n",
      "  4.37102653e-06 6.09286781e-06]\n",
      " [7.85899699e-01 2.20572467e-08 1.90806577e-06 ... 1.51370500e-07\n",
      "  2.85213673e-06 3.44994078e-06]\n",
      " [9.86455917e-01 2.97802716e-09 2.82061791e-07 ... 1.43518593e-08\n",
      "  2.61086885e-08 1.13456355e-08]\n",
      " ...\n",
      " [9.99976516e-01 1.48177166e-13 6.82855786e-11 ... 9.91736315e-13\n",
      "  1.87624677e-12 7.45158763e-13]\n",
      " [9.99981046e-01 1.43173356e-13 6.00178726e-11 ... 7.46433839e-13\n",
      "  1.37678291e-12 6.22006841e-13]\n",
      " [9.99978065e-01 2.48328485e-13 6.99186334e-11 ... 9.38577232e-13\n",
      "  1.82155766e-12 7.41060804e-13]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-5b71540e2a1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes_answer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices_vocabA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'#end'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# fin de la oracion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "def predict_words(model, example, diversity):\n",
    "  model = load_model('attention_model.h5')\n",
    "  example = np.array(example)\n",
    "  example.reshape((60,))\n",
    "  prediction = model.predict(example)\n",
    "  return prediction\n",
    "  \n",
    "n = 10\n",
    "for i in range(n):\n",
    "  indexs = np.random.randint(0, len(X_test_Q)-2)\n",
    "  example = X_train_Q[indexs:(indexs+1)]\n",
    "  indexes_answer = predict_words(model, example, 0.85)\n",
    "  question = test['question'][indexs]\n",
    "  print('Pregunta: ', question)\n",
    "  answer = ''\n",
    "  for index in indexes_answer:\n",
    "    print(index)\n",
    "    if (indices_vocabA[index] == '#end'): # fin de la oracion\n",
    "      continue\n",
    "    else:\n",
    "      answer += indices_vocabA[index]+' '\n",
    "  print('Respuesta: ', answer)\n",
    "print('Los ha predecido todos!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8zMagSuqsp1"
   },
   "source": [
    "## h) Evaluacion del modelo\n",
    "Para verificar la calidad del modelo, compararemos con el benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ffqf1bH-qsoI"
   },
   "outputs": [],
   "source": [
    "!python evaluate-v2.0.py dev-v2.0.json predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_awuqAZqslg"
   },
   "outputs": [],
   "source": [
    "dic_predictions = {}\n",
    "for example, id_e in zip(Xtest_question, df_test[\"id\"]): # todos los ejemplos\n",
    "  indexes_answer = predict_words(model, example) # predice palabra en cada instante\n",
    "  answer = \"\"\n",
    "  for index in indexes_answer:\n",
    "    if(indices_vocabA[index] == '#end'): # Final de la oracion\n",
    "      continue\n",
    "    else:\n",
    "      answer += indices_vocabA[index]+\" \"\n",
    "  dic_predictions[id_e] = answer\n",
    "  contador += 1\n",
    "  print('Los ha predecido todos!')\n",
    "  json_save = json.dumps(dic_predictions)\n",
    "  archivo = open('predictions', 'w')\n",
    "  archivo.write(json.save)\n",
    "  archivo.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "P2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
